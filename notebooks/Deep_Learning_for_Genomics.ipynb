{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true,
    "id": "UE_b7OpncLwm"
   },
   "source": [
    "# Predicting TF binding in DNA sequences using convolutional networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import math \n",
    "import random\n",
    "import gzip\n",
    "import torch\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import os \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true,
    "id": "fSTmS7206gMF"
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hideCode": true,
    "hidePrompt": true,
    "id": "Cktrf76C6no6"
   },
   "outputs": [],
   "source": [
    "# datasets can be obtained e.g. from:\n",
    "# https://github.com/MedChaabane/deepRAM/tree/master/datasets/ChIP-seq\n",
    "\n",
    "def seqtopad(sequence,motlen):\n",
    "    rows=len(sequence)+2*motlen-2\n",
    "    S=np.empty([rows,4])\n",
    "    base=['A', 'C', 'G', 'T']\n",
    "    for i in range(rows):\n",
    "        for j in range(4):\n",
    "            if (i-motlen+1<len(sequence) and sequence[i-motlen+1]=='N' \n",
    "                or i<motlen-1 or i>len(sequence)+motlen-2):\n",
    "                S[i,j]=np.float32(0.25)\n",
    "            elif sequence[i-motlen+1]==base[j]:\n",
    "                S[i,j]=np.float32(1)\n",
    "            else:\n",
    "                S[i,j]=np.float32(0)\n",
    "    return np.transpose(S)\n",
    "\n",
    "def openFile(path, motiflen=24):\n",
    "        train_dataset=[]\n",
    "        sequences=[]\n",
    "        with gzip.open(path, 'rt') as data:\n",
    "                next(data)\n",
    "                reader = csv.reader(data,delimiter='\\t')\n",
    "                for row in reader:\n",
    "                    train_dataset.append(\n",
    "                        [seqtopad(row[2],motiflen),[int(row[3])]])\n",
    "                    sequences.append(row[2])\n",
    "  \n",
    "        random.shuffle(train_dataset)\n",
    "        size=int(len(train_dataset)/5)\n",
    "        firsttrain=train_dataset[:4*size]\n",
    "        firstvalid=train_dataset[4*size+1:]\n",
    "        validseq = sequences[4*size+1:]\n",
    "\n",
    "        return firsttrain,firstvalid,train_dataset,sequences,validseq\n",
    "\n",
    "class chipseq_dataset(Dataset):\n",
    "    def __init__(self,xy=None):\n",
    "        self.x_data=np.asarray([el[0] for el in xy],dtype=np.float32)\n",
    "        self.y_data =np.asarray([el[1] for el in xy ],dtype=np.float32)\n",
    "        self.x_data = torch.from_numpy(self.x_data)\n",
    "        self.y_data = torch.from_numpy(self.y_data)\n",
    "        self.len=len(self.x_data)\n",
    "      \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "train1,valid1,alldataset,sequences,seq_motif=openFile(\n",
    "    \"SRF_H1-hESC_SRF_HudsonAlpha_B.seq.gz\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "train1_dataset=chipseq_dataset(train1)\n",
    "valid1_dataset=chipseq_dataset(valid1)\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(dataset=train1_dataset,\n",
    "                          batch_size=batch_size,shuffle=True)\n",
    "valid_loader = DataLoader(dataset=valid1_dataset,\n",
    "                          batch_size=batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true,
    "id": "cB4zDDTWgttB"
   },
   "source": [
    "## Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "class DeepBind(nn.Module):\n",
    "    def __init__(self, nummotif, motiflen,\n",
    "                 sigmaConv, sigmaNeu):\n",
    "      \n",
    "        super(DeepBind, self).__init__()\n",
    "        self.sigmaConv=sigmaConv\n",
    "        self.sigmaNeu=sigmaNeu\n",
    "        self.input_channels=4\n",
    "        self.activation = nn.ReLU()\n",
    "        self.conv_weights=torch.randn(nummotif,\n",
    "                                      self.input_channels, \n",
    "                                      motiflen).to(device)\n",
    "        self.conv_bias=torch.randn(nummotif).to(device)\n",
    "        self.FC_size=nummotif\n",
    "        torch.nn.init.normal_(self.conv_weights, mean=0,\n",
    "                              std=sigmaConv)\n",
    "        torch.nn.init.normal_(self.conv_bias)        \n",
    "        self.conv_weights.requires_grad=True\n",
    "        self.conv_bias.requires_grad=True\n",
    "        \n",
    "        self.wNeu=torch.randn(self.FC_size,1).to(device)\n",
    "        self.wNeuBias=torch.randn(1).to(device)\n",
    "        torch.nn.init.normal_(self.wNeu,mean=0,\n",
    "                              std=self.sigmaNeu)\n",
    "        torch.nn.init.normal_(self.wNeuBias,mean=0,\n",
    "                              std=self.sigmaNeu)\n",
    "        self.wNeu.requires_grad=True\n",
    "        self.wNeuBias.requires_grad=True\n",
    "                \n",
    "    def get_weights(self):\n",
    "        return [self.conv_weights, self.conv_bias, model.wNeu, model.wNeuBias]\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 1d convolution:\n",
    "        x=F.conv1d(x, self.conv_weights, bias=self.conv_bias, \n",
    "                   stride=1, padding=0)\n",
    "        # activation\n",
    "        x=self.activation(x)\n",
    "        # max pooling\n",
    "        x,_ = torch.max(x, dim=2)\n",
    "        # fully connected layer\n",
    "        x=x@self.wNeu + self.wNeuBias\n",
    "        return torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hideCode": true,
    "hidePrompt": true,
    "id": "GECeyDzekUWN",
    "outputId": "af128073-3bcd-4167-9ba8-3e7d87c78bc2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepBind(\n",
      "  (activation): ReLU()\n",
      ")\n",
      "tensor(0.6932, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.6906, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.6927, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.6939, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.6867, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.6004, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.4615, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.3564, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.3093, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.3564, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.4020, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.3672, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.3877, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.2545, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.2914, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.2541, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.3287, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.1930, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.2173, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.1779, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.1134, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.1705, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.1489, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.1370, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.1244, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.1100, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.1034, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.1560, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.1598, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.0659, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.0887, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.0756, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.0901, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.0854, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.0678, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.0680, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.0421, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.0568, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.1035, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.0706, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.0621, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.0435, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.0401, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.0658, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.0417, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.0504, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.0371, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.0491, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.0286, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.0287, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.0354, grad_fn=<BinaryCrossEntropyBackward>)\n"
     ]
    }
   ],
   "source": [
    "device = 'cpu'\n",
    "model = DeepBind(16,14,1e-06,0.001).to(device)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.get_weights(),\n",
    "                            lr=0.01,momentum=0.9,nesterov=True,\n",
    "                            weight_decay=3e-06)\n",
    "\n",
    "import torch.nn.functional as F\n",
    "learning_steps=0\n",
    "model.train()\n",
    "print(model)\n",
    "while learning_steps<=5000:\n",
    "    for i, (data, target) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        output = model(data)\n",
    "        loss = F.binary_cross_entropy(output, target)\n",
    "        if learning_steps%100==0 :\n",
    "            print(loss)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        learning_steps+=1\n",
    "\n",
    "torch.save(model, 'best_model.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on validation data  0.9078787878787878\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    pred_list = []\n",
    "    labels_list = []\n",
    "    for i, (data, target) in enumerate(valid_loader):\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        output = model(data)\n",
    "        pred=output.cpu().detach().numpy().reshape(output.shape[0])\n",
    "        labels=target.cpu().numpy().reshape(output.shape[0])\n",
    "        pred_list.append(pred)\n",
    "        labels_list.append(labels)\n",
    "    labels = np.concatenate(labels_list)\n",
    "    predictions = np.concatenate(pred_list)\n",
    "    auc = metrics.roc_auc_score(labels, predictions)\n",
    "    print('AUC on validation data ', auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.conv_weights[0]"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Deep Learning for Genomics.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "hide_code_all_hidden": true,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
